{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95552135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.network_swinir import *\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc609bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_oom_mr_image(path):\n",
    "    \"\"\"\n",
    "    Read an MR image, convert to RGB, resize to 500×500, \n",
    "    and generate a noisy low-quality version.\n",
    "    Returns (imgname, img_lq, img_gt)\n",
    "    \"\"\"\n",
    "    # --- get name ---\n",
    "    imgname, _ = os.path.splitext(os.path.basename(path))\n",
    "\n",
    "    # --- read image ---\n",
    "    img_gt = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    img_gt = cv2.cvtColor(img_gt, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # --- resize to 500×500 ---\n",
    "    img_gt = cv2.resize(img_gt, (500, 500), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # --- normalize to [0, 1] ---\n",
    "    img_gt = img_gt.astype(np.float32) / 255.0\n",
    "\n",
    "    # --- generate noisy version (LQ) ---\n",
    "    np.random.seed(0)\n",
    "    noise = np.random.normal(0, 50 / 255., img_gt.shape)\n",
    "    img_lq = np.clip(img_gt + noise, 0, 1)\n",
    "\n",
    "    return imgname, img_lq, img_gt\n",
    "\n",
    "def get_image_pair(path):\n",
    "    (imgname, imgext) = os.path.splitext(os.path.basename(path))\n",
    "    img_gt = cv2.imread(path, cv2.IMREAD_COLOR).astype(np.float32) / 255.\n",
    "    np.random.seed(seed=0)\n",
    "    img_lq = img_gt + np.random.normal(0, 50 / 255., img_gt.shape)\n",
    "\n",
    "    return imgname, img_lq, img_gt\n",
    "\n",
    "def show_three_images(img_lq, img_gt, output):\n",
    "    # resize to 256×256\n",
    "    img_lq_resized = TF.resize(img_lq, [64, 64])\n",
    "    img_gt_resized = TF.resize(img_gt, [64, 64])\n",
    "    output_resized = TF.resize(output, [64, 64])\n",
    "\n",
    "    # remove batch dimension and convert to numpy\n",
    "    imgs = [img_lq_resized[0], img_gt_resized[0], output_resized[0]]\n",
    "    imgs = [img.permute(1, 2, 0).detach().cpu().numpy() for img in imgs]\n",
    "\n",
    "    # plot in a row\n",
    "    titles = ['LQ', 'GT', 'Output']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    for ax, img, title in zip(axes, imgs, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def calculate_psnr(img1, img2, crop_border, input_order='HWC', test_y_channel=False):\n",
    "    \"\"\"Calculate PSNR (Peak Signal-to-Noise Ratio).\n",
    "\n",
    "    Ref: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
    "\n",
    "    Args:\n",
    "        img1 (ndarray): Images with range [0, 255].\n",
    "        img2 (ndarray): Images with range [0, 255].\n",
    "        crop_border (int): Cropped pixels in each edge of an image. These\n",
    "            pixels are not involved in the PSNR calculation.\n",
    "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
    "            Default: 'HWC'.\n",
    "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        float: psnr result.\n",
    "    \"\"\"\n",
    "\n",
    "    assert img1.shape == img2.shape, (f'Image shapes are differnet: {img1.shape}, {img2.shape}.')\n",
    "    if input_order not in ['HWC', 'CHW']:\n",
    "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' '\"HWC\" and \"CHW\"')\n",
    "    img1 = reorder_image(img1, input_order=input_order)\n",
    "    img2 = reorder_image(img2, input_order=input_order)\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "\n",
    "    if crop_border != 0:\n",
    "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "\n",
    "    if test_y_channel:\n",
    "        img1 = to_y_channel(img1)\n",
    "        img2 = to_y_channel(img2)\n",
    "\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20. * np.log10(255. / np.sqrt(mse))\n",
    "\n",
    "\n",
    "def _ssim(img1, img2):\n",
    "    \"\"\"Calculate SSIM (structural similarity) for one channel images.\n",
    "\n",
    "    It is called by func:`calculate_ssim`.\n",
    "\n",
    "    Args:\n",
    "        img1 (ndarray): Images with range [0, 255] with order 'HWC'.\n",
    "        img2 (ndarray): Images with range [0, 255] with order 'HWC'.\n",
    "\n",
    "    Returns:\n",
    "        float: ssim result.\n",
    "    \"\"\"\n",
    "\n",
    "    C1 = (0.01 * 255) ** 2\n",
    "    C2 = (0.03 * 255) ** 2\n",
    "\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "\n",
    "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]\n",
    "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
    "    mu1_sq = mu1 ** 2\n",
    "    mu2_sq = mu2 ** 2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = cv2.filter2D(img1 ** 2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2 ** 2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "\n",
    "def calculate_ssim(img1, img2, crop_border, input_order='HWC', test_y_channel=False):\n",
    "    \"\"\"Calculate SSIM (structural similarity).\n",
    "\n",
    "    Ref:\n",
    "    Image quality assessment: From error visibility to structural similarity\n",
    "\n",
    "    The results are the same as that of the official released MATLAB code in\n",
    "    https://ece.uwaterloo.ca/~z70wang/research/ssim/.\n",
    "\n",
    "    For three-channel images, SSIM is calculated for each channel and then\n",
    "    averaged.\n",
    "\n",
    "    Args:\n",
    "        img1 (ndarray): Images with range [0, 255].\n",
    "        img2 (ndarray): Images with range [0, 255].\n",
    "        crop_border (int): Cropped pixels in each edge of an image. These\n",
    "            pixels are not involved in the SSIM calculation.\n",
    "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
    "            Default: 'HWC'.\n",
    "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        float: ssim result.\n",
    "    \"\"\"\n",
    "\n",
    "    assert img1.shape == img2.shape, (f'Image shapes are differnet: {img1.shape}, {img2.shape}.')\n",
    "    if input_order not in ['HWC', 'CHW']:\n",
    "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' '\"HWC\" and \"CHW\"')\n",
    "    img1 = reorder_image(img1, input_order=input_order)\n",
    "    img2 = reorder_image(img2, input_order=input_order)\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "\n",
    "    if crop_border != 0:\n",
    "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "\n",
    "    if test_y_channel:\n",
    "        img1 = to_y_channel(img1)\n",
    "        img2 = to_y_channel(img2)\n",
    "\n",
    "    ssims = []\n",
    "    for i in range(img1.shape[2]):\n",
    "        ssims.append(_ssim(img1[..., i], img2[..., i]))\n",
    "    return np.array(ssims).mean()\n",
    "\n",
    "\n",
    "def _blocking_effect_factor(im):\n",
    "    block_size = 8\n",
    "\n",
    "    block_horizontal_positions = torch.arange(7, im.shape[3] - 1, 8)\n",
    "    block_vertical_positions = torch.arange(7, im.shape[2] - 1, 8)\n",
    "\n",
    "    horizontal_block_difference = (\n",
    "                (im[:, :, :, block_horizontal_positions] - im[:, :, :, block_horizontal_positions + 1]) ** 2).sum(\n",
    "        3).sum(2).sum(1)\n",
    "    vertical_block_difference = (\n",
    "                (im[:, :, block_vertical_positions, :] - im[:, :, block_vertical_positions + 1, :]) ** 2).sum(3).sum(\n",
    "        2).sum(1)\n",
    "\n",
    "    nonblock_horizontal_positions = np.setdiff1d(torch.arange(0, im.shape[3] - 1), block_horizontal_positions)\n",
    "    nonblock_vertical_positions = np.setdiff1d(torch.arange(0, im.shape[2] - 1), block_vertical_positions)\n",
    "\n",
    "    horizontal_nonblock_difference = (\n",
    "                (im[:, :, :, nonblock_horizontal_positions] - im[:, :, :, nonblock_horizontal_positions + 1]) ** 2).sum(\n",
    "        3).sum(2).sum(1)\n",
    "    vertical_nonblock_difference = (\n",
    "                (im[:, :, nonblock_vertical_positions, :] - im[:, :, nonblock_vertical_positions + 1, :]) ** 2).sum(\n",
    "        3).sum(2).sum(1)\n",
    "\n",
    "    n_boundary_horiz = im.shape[2] * (im.shape[3] // block_size - 1)\n",
    "    n_boundary_vert = im.shape[3] * (im.shape[2] // block_size - 1)\n",
    "    boundary_difference = (horizontal_block_difference + vertical_block_difference) / (\n",
    "                n_boundary_horiz + n_boundary_vert)\n",
    "\n",
    "    n_nonboundary_horiz = im.shape[2] * (im.shape[3] - 1) - n_boundary_horiz\n",
    "    n_nonboundary_vert = im.shape[3] * (im.shape[2] - 1) - n_boundary_vert\n",
    "    nonboundary_difference = (horizontal_nonblock_difference + vertical_nonblock_difference) / (\n",
    "                n_nonboundary_horiz + n_nonboundary_vert)\n",
    "\n",
    "    scaler = np.log2(block_size) / np.log2(min([im.shape[2], im.shape[3]]))\n",
    "    bef = scaler * (boundary_difference - nonboundary_difference)\n",
    "\n",
    "    bef[boundary_difference <= nonboundary_difference] = 0\n",
    "    return bef\n",
    "\n",
    "\n",
    "def calculate_psnrb(img1, img2, crop_border, input_order='HWC', test_y_channel=False):\n",
    "    \"\"\"Calculate PSNR-B (Peak Signal-to-Noise Ratio).\n",
    "\n",
    "    Ref: Quality assessment of deblocked images, for JPEG image deblocking evaluation\n",
    "    # https://gitlab.com/Queuecumber/quantization-guided-ac/-/blob/master/metrics/psnrb.py\n",
    "\n",
    "    Args:\n",
    "        img1 (ndarray): Images with range [0, 255].\n",
    "        img2 (ndarray): Images with range [0, 255].\n",
    "        crop_border (int): Cropped pixels in each edge of an image. These\n",
    "            pixels are not involved in the PSNR calculation.\n",
    "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
    "            Default: 'HWC'.\n",
    "        test_y_channel (bool): Test on Y channel of YCbCr. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        float: psnr result.\n",
    "    \"\"\"\n",
    "\n",
    "    assert img1.shape == img2.shape, (f'Image shapes are differnet: {img1.shape}, {img2.shape}.')\n",
    "    if input_order not in ['HWC', 'CHW']:\n",
    "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' '\"HWC\" and \"CHW\"')\n",
    "    img1 = reorder_image(img1, input_order=input_order)\n",
    "    img2 = reorder_image(img2, input_order=input_order)\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "\n",
    "    if crop_border != 0:\n",
    "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, ...]\n",
    "\n",
    "    if test_y_channel:\n",
    "        img1 = to_y_channel(img1)\n",
    "        img2 = to_y_channel(img2)\n",
    "\n",
    "    # follow https://gitlab.com/Queuecumber/quantization-guided-ac/-/blob/master/metrics/psnrb.py\n",
    "    img1 = torch.from_numpy(img1).permute(2, 0, 1).unsqueeze(0) / 255.\n",
    "    img2 = torch.from_numpy(img2).permute(2, 0, 1).unsqueeze(0) / 255.\n",
    "\n",
    "    total = 0\n",
    "    for c in range(img1.shape[1]):\n",
    "        mse = torch.nn.functional.mse_loss(img1[:, c:c + 1, :, :], img2[:, c:c + 1, :, :], reduction='none')\n",
    "        bef = _blocking_effect_factor(img1[:, c:c + 1, :, :])\n",
    "\n",
    "        mse = mse.view(mse.shape[0], -1).mean(1)\n",
    "        total += 10 * torch.log10(1 / (mse + bef))\n",
    "\n",
    "    return float(total) / img1.shape[1]\n",
    "\n",
    "\n",
    "def reorder_image(img, input_order='HWC'):\n",
    "    \"\"\"Reorder images to 'HWC' order.\n",
    "\n",
    "    If the input_order is (h, w), return (h, w, 1);\n",
    "    If the input_order is (c, h, w), return (h, w, c);\n",
    "    If the input_order is (h, w, c), return as it is.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): Input image.\n",
    "        input_order (str): Whether the input order is 'HWC' or 'CHW'.\n",
    "            If the input image shape is (h, w), input_order will not have\n",
    "            effects. Default: 'HWC'.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: reordered image.\n",
    "    \"\"\"\n",
    "\n",
    "    if input_order not in ['HWC', 'CHW']:\n",
    "        raise ValueError(f'Wrong input_order {input_order}. Supported input_orders are ' \"'HWC' and 'CHW'\")\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[..., None]\n",
    "    if input_order == 'CHW':\n",
    "        img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def to_y_channel(img):\n",
    "    \"\"\"Change to Y channel of YCbCr.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): Images with range [0, 255].\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): Images with range [0, 255] (float type) without round.\n",
    "    \"\"\"\n",
    "    img = img.astype(np.float32) / 255.\n",
    "    if img.ndim == 3 and img.shape[2] == 3:\n",
    "        img = bgr2ycbcr(img, y_only=True)\n",
    "        img = img[..., None]\n",
    "    return img * 255.\n",
    "\n",
    "\n",
    "def _convert_input_type_range(img):\n",
    "    \"\"\"Convert the type and range of the input image.\n",
    "\n",
    "    It converts the input image to np.float32 type and range of [0, 1].\n",
    "    It is mainly used for pre-processing the input image in colorspace\n",
    "    convertion functions such as rgb2ycbcr and ycbcr2rgb.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): The converted image with type of np.float32 and range of\n",
    "            [0, 1].\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = img.astype(np.float32)\n",
    "    if img_type == np.float32:\n",
    "        pass\n",
    "    elif img_type == np.uint8:\n",
    "        img /= 255.\n",
    "    else:\n",
    "        raise TypeError('The img type should be np.float32 or np.uint8, ' f'but got {img_type}')\n",
    "    return img\n",
    "\n",
    "\n",
    "def _convert_output_type_range(img, dst_type):\n",
    "    \"\"\"Convert the type and range of the image according to dst_type.\n",
    "\n",
    "    It converts the image to desired type and range. If `dst_type` is np.uint8,\n",
    "    images will be converted to np.uint8 type with range [0, 255]. If\n",
    "    `dst_type` is np.float32, it converts the image to np.float32 type with\n",
    "    range [0, 1].\n",
    "    It is mainly used for post-processing images in colorspace convertion\n",
    "    functions such as rgb2ycbcr and ycbcr2rgb.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): The image to be converted with np.float32 type and\n",
    "            range [0, 255].\n",
    "        dst_type (np.uint8 | np.float32): If dst_type is np.uint8, it\n",
    "            converts the image to np.uint8 type with range [0, 255]. If\n",
    "            dst_type is np.float32, it converts the image to np.float32 type\n",
    "            with range [0, 1].\n",
    "\n",
    "    Returns:\n",
    "        (ndarray): The converted image with desired type and range.\n",
    "    \"\"\"\n",
    "    if dst_type not in (np.uint8, np.float32):\n",
    "        raise TypeError('The dst_type should be np.float32 or np.uint8, ' f'but got {dst_type}')\n",
    "    if dst_type == np.uint8:\n",
    "        img = img.round()\n",
    "    else:\n",
    "        img /= 255.\n",
    "    return img.astype(dst_type)\n",
    "\n",
    "\n",
    "def bgr2ycbcr(img, y_only=False):\n",
    "    \"\"\"Convert a BGR image to YCbCr image.\n",
    "\n",
    "    The bgr version of rgb2ycbcr.\n",
    "    It implements the ITU-R BT.601 conversion for standard-definition\n",
    "    television. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#ITU-R_BT.601_conversion.\n",
    "\n",
    "    It differs from a similar function in cv2.cvtColor: `BGR <-> YCrCb`.\n",
    "    In OpenCV, it implements a JPEG conversion. See more details in\n",
    "    https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion.\n",
    "\n",
    "    Args:\n",
    "        img (ndarray): The input image. It accepts:\n",
    "            1. np.uint8 type with range [0, 255];\n",
    "            2. np.float32 type with range [0, 1].\n",
    "        y_only (bool): Whether to only return Y channel. Default: False.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: The converted YCbCr image. The output image has the same type\n",
    "            and range as input image.\n",
    "    \"\"\"\n",
    "    img_type = img.dtype\n",
    "    img = _convert_input_type_range(img)\n",
    "    if y_only:\n",
    "        out_img = np.dot(img, [24.966, 128.553, 65.481]) + 16.0\n",
    "    else:\n",
    "        out_img = np.matmul(\n",
    "            img, [[24.966, 112.0, -18.214], [128.553, -74.203, -93.786], [65.481, -37.797, 112.0]]) + [16, 128, 128]\n",
    "    out_img = _convert_output_type_range(out_img, img_type)\n",
    "    return out_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/home/data1/musong/workspace/python/SwinIR/model_zoo/swinir/005_colorDN_DFWB_s128w8_SwinIR-M_noise50.pth\"\n",
    "folder_gt = \"/home/data1/musong/workspace/python/SwinIR/testsets/McMaster\"\n",
    "mr_img_path = '/home/data1/musong/data/fastmri/diffusion_images/file_brain_AXT2_208_2080298_3.jpg'\n",
    "border = 0\n",
    "window_size = 8\n",
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e78ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinIR(upscale=1, in_chans=3, img_size=128, window_size=8,\n",
    "            img_range=1., depths=[6, 6, 6, 6, 6, 6], embed_dim=180, num_heads=[6, 6, 6, 6, 6, 6],\n",
    "            mlp_ratio=2, upsampler='', resi_connection='1conv')\n",
    "\n",
    "pretrained_model = torch.load(ckpt_path)\n",
    "param_key_g = 'params'\n",
    "model.load_state_dict(pretrained_model[param_key_g] if param_key_g in pretrained_model.keys() else pretrained_model, strict=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594dd6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_image_pair(f'{folder_gt}/1.tif')\n",
    "img_lq = data[1]\n",
    "img_gt = data[2]\n",
    "\n",
    "# _, img_lq, img_gt = prepare_oom_mr_image(mr_img_path)\n",
    "\n",
    "img_lq = np.transpose(img_lq if img_lq.shape[2] == 1 else img_lq[:, :, [2, 1, 0]], (2, 0, 1)) \n",
    "img_lq = torch.from_numpy(img_lq).float().unsqueeze(0).to(device) \n",
    "img_gt = np.transpose(img_gt if img_gt.shape[2] == 1 else img_gt[:, :, [2, 1, 0]], (2, 0, 1)) \n",
    "img_gt = torch.from_numpy(img_gt).float().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pad input image to be a multiple of window_size\n",
    "    _, _, h_old, w_old = img_lq.size()\n",
    "    h_pad = (h_old // window_size + 1) * window_size - h_old\n",
    "    w_pad = (w_old // window_size + 1) * window_size - w_old\n",
    "    # img_lq = torch.cat([img_lq, torch.flip(img_lq, [2])], 2)[:, :, :h_old + h_pad, :]\n",
    "    # img_lq = torch.cat([img_lq, torch.flip(img_lq, [3])], 3)[:, :, :, :w_old + w_pad]\n",
    "\n",
    "    output = model(img_lq)\n",
    "\n",
    "# # free up GPU tensors explicitly\n",
    "# del img_lq, output\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f795c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_three_images(img_lq, img_gt, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gt = (img_gt * 255.0).round().detach().cpu().numpy().astype(np.uint8) \n",
    "img_lq = (img_lq * 255.0).round().detach().cpu().numpy().astype(np.uint8) \n",
    "output = (output * 255.0).round().detach().cpu().numpy().astype(np.uint8) \n",
    "print(calculate_psnr(img_lq, img_gt, crop_border=0))\n",
    "print(calculate_psnr(output, img_gt, crop_border=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
